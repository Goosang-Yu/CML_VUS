import sys, os
import pandas as pd
import numpy as np
from tqdm import tqdm
from glob import glob
from scipy.stats import fisher_exact
from scipy import stats
from moepy import lowess

def make_count_file(freq_table:str, var_ref:str) -> pd.DataFrame:
    """Using CRISPResso2 to extract read counts for each variant from the alignment file of reads.
    
    Args:
        freq_table (str): Path to the frequency table file generated by CRISPResso.
        var_ref (str): Path to the reference file containing the sequence with the variants to be analyzed and information about those variants.

    Returns:
        pd.DataFrame: _description_
    """    
    
    # Step1: read CRISPResso aligned & reference file
    df_ref = pd.read_csv(var_ref)
    df = pd.read_csv(freq_table, sep = '\t')
    
    sample_name = os.path.basename(freq_table).replace('.txt', '')
    dict_out = {}

    for ref_seq in df_ref['RefSeq']:
        dict_out[ref_seq]  = 0

    # Step2: read count
    for i in tqdm(df.index, desc=f'[Info] Read counting: {sample_name}'):
        data = df.iloc[i]
        seq  = data['Aligned_Sequence']
        cnt  = int(data['#Reads'])
        
        try   : dict_out[seq] += cnt
        except: dict_out['No_matched'] += cnt

    # Step3: make output
    list_count = []

    for ref_seq in df_ref['RefSeq']:
        list_count.append(dict_out[ref_seq])

    df_out = df_ref.copy()
    df_out['count'] = list_count

    total_cnt = df_out['count'].sum()
    df_out['frequency'] = [cnt/total_cnt for cnt in df_out['count']]

    return df_out



def read_statistics(var_control:str, background:str, hit_label:str='SynPE', adjustment:str='bonferroni') -> pd.DataFrame:
    """_summary_

    Args:
        var_control (str): _description_
        background (str): _description_

    Raises:
        ValueError: _description_

    Returns:
        pd.DataFrame: _description_
    """    


    # Load DataFrame
    df_test = pd.read_csv(var_control)
    df_UE   = pd.read_csv(background)

    ## Check if the variants list of var_control matches exactly with that of the background!
    if False in list(df_UE['RefSeq'] == df_test['RefSeq']):
        raise ValueError('Not matched between sample and background. Please check your input files.')
    
    UE_WT_read  = df_UE[df_UE['Label']=='WT_refseq']['count'].iloc[0]
    UE_SynPE_df = df_UE[df_UE['Label']==hit_label].reset_index(drop=True)

    # Step1: Make unedit dictionary
    UE_SynPE_dict = {}

    for i in UE_SynPE_df.index:
        UE_data = UE_SynPE_df.iloc[i]
        UE_SynPE_dict[UE_data['RefSeq']] = int(UE_data['count'])

    # Step2: Add odds/p-value column to each Stat file.

    f_name = os.path.basename(var_control).replace('.csv', '')
    print('Analysis:', f_name)
    
    df_synpe  = df_test[df_test['Label']==hit_label].reset_index(drop=True).copy()

    total_cnt_wtseq = df_test[df_test['Label']=='WT_refseq']['count'].iloc[0]
    total_cnt_edseq = df_synpe['count'].sum()

    list_sample_WT  = [] # list_sample_wt_cnt
    list_sample_RPM = []
    list_UE_SynPE   = []
    list_UE_WT_read = []
    list_odds_ratio = []
    list_pvalue     = []

    for i in df_synpe.index:

        data = df_synpe.loc[i]

        sample_SynPE_cnt = int(df_synpe.loc[i]['count'])
        sample_SynPE_rpm = sample_SynPE_cnt*1000000/total_cnt_edseq
        unedit_SynPE_cnt = UE_SynPE_dict[data['RefSeq']]

        # Calculate odds ratio
        odds = ((sample_SynPE_cnt+1)/(total_cnt_wtseq+1))/((unedit_SynPE_cnt+1)/(UE_WT_read+1))
        
        # Calculate Fisher test p-value
        fisher_df = pd.DataFrame({'intended_edit':[sample_SynPE_cnt, unedit_SynPE_cnt], 'WT':[total_cnt_wtseq, UE_WT_read]})
        oddsr, p = fisher_exact(table = fisher_df.to_numpy(), alternative = 'two-sided')

        # Store the calculated values in a list.
        list_sample_WT.append(total_cnt_wtseq)
        list_sample_RPM.append(sample_SynPE_rpm)
        list_UE_SynPE.append(unedit_SynPE_cnt)
        list_UE_WT_read.append(UE_WT_read)
        list_odds_ratio.append(odds)
        list_pvalue.append(p)

    df_synpe['Edited_WT_count'] = list_sample_WT
    df_synpe['RPM']             = list_sample_RPM
    df_synpe['UE_SynPE_count']  = list_UE_SynPE
    df_synpe['UE_WT_count']     = list_UE_WT_read
    df_synpe['OR']              = list_odds_ratio
    df_synpe['pvalue']          = list_pvalue

    if adjustment == 'bonferroni':
        df_synpe['adj_pvalue'] = [len(df_synpe)*pv for pv in list_pvalue]

    return df_synpe


class VariantFilter:
    def __init__(self, test_r1:str, test_r2:str, control_r1:str, control_r2:str):
        """A function to generate filtered test data based on odds ratio/p-value criteria.
        Always assumes there are replicates (two experimental groups).

        Args:
            test_r1 (str): Read count file, replicate 1
            test_r2 (str): Read count file, replicate 2
            control_r1 (str): Control statistics containing odds ratio and fisher's t-test p-value, replicate 1
            control_r2 (str): Control statistics containing odds ratio and fisher's t-test p-value, replicate 2

        Raises:
            FileNotFoundError: Not proper input file path.
        """        

        try: 
            self.df1_test = pd.read_csv(test_r1)
            self.df2_test = pd.read_csv(test_r2)
            
            self.df1_control = pd.read_csv(control_r1)
            self.df2_control = pd.read_csv(control_r2)

        except:
            raise FileNotFoundError('Not found statistics data. Please check your input.')


    def filter(self, hit_label:str='SynPE', OR_cutoff: float = 2, p_cutoff: float = 0.05, p_column:str='adj_pvalue', rpm_cutoff:float=10) -> pd.DataFrame:
        """It takes Stats files containing odds ratio and Fisher's t-test p-values as input. Based on this, it filters variant reads.
        Then, it performs LOWESS regression using the log2-fold change (LFC) of synonymous mutations.
        After correcting the LFC with LOWESS regression, it calculates normalized LFC and returns a DataFrame containing their information.

        Args:
            hit_label (str, optional): _description_. Defaults to 'SynPE'.
            OR_cutoff (float, optional): _description_. Defaults to 2.
            p_cutoff (float, optional): _description_. Defaults to 0.05.
            p_column (str, optional): _description_. Defaults to 'adj_pvalue'.
            rpm_cutoff (float, optional): _description_. Defaults to 10.

        Returns:
            pd.DataFrame: _description_
        """        

        df1_test = self.df1_test.copy()
        df2_test = self.df2_test.copy()
        
        df1_control = self.df1_control.copy()
        df2_control = self.df2_control.copy()


        # Step1: Selects only the mutations that emerge consistently in both Replicate 1 (R1) and Replicate 2 (R2) 
        # among the sequences exceeding the OR/p-value cutoff and inducing mutations in the coding sequence (CDS).

        df1_filtered = df1_control[(df1_control['OR']>OR_cutoff) & (df1_control[p_column]<p_cutoff) & (df1_control['RPM']>=rpm_cutoff)].copy()
        df2_filtered = df2_control[(df2_control['OR']>OR_cutoff) & (df2_control[p_column]<p_cutoff) & (df2_control['RPM']>=rpm_cutoff)].copy()

        # Ensures that only those passing through the intersection of Replicate 1 and Replicate 2 are displayed.
        filtered_RefSeq = list(set(df1_filtered['RefSeq']) & set(df2_filtered['RefSeq']))

        df1_filtered = df1_filtered[df1_filtered['RefSeq'].isin(filtered_RefSeq)].copy()
        df2_filtered = df2_filtered[df2_filtered['RefSeq'].isin(filtered_RefSeq)].copy()

        df1_filtered = df1_filtered[['RefSeq', 'AA_var', 'SNV_var', 'RPM']]
        df2_filtered = df2_filtered[['RefSeq', 'AA_var', 'SNV_var', 'RPM']]

        df1_filtered.columns = ['RefSeq', 'AA_var', 'SNV_var', 'control']
        df2_filtered.columns = ['RefSeq', 'AA_var', 'SNV_var', 'control']


        # Step 2: Retrieves read counts only for the sequences that pass the cutoff and formats them into a Mageck count file.
        df1_test_Syn = df1_test[df1_test['Label']==hit_label].copy()
        df1_test_Syn['RPM'] = df1_test_Syn['count'] * 1000000 / np.sum(df1_test_Syn['count'])
        df1_test_dict = dict(zip(df1_test_Syn['RefSeq'], df1_test_Syn['RPM']))
        df1_filtered['test'] = [df1_test_dict.get(i) for i in df1_filtered['RefSeq']]
        df1_filtered.dropna(axis = 0, inplace=True)

        df2_test_Syn = df2_test[df2_test['Label']==hit_label].copy()
        df2_test_Syn['RPM'] = df2_test_Syn['count'] * 1000000 / np.sum(df2_test_Syn['count'])
        df2_test_dict = dict(zip(df2_test_Syn['RefSeq'], df2_test_Syn['RPM']))
        df2_filtered['test'] = [df2_test_dict.get(i) for i in df2_filtered['RefSeq']]
        df2_filtered.dropna(axis = 0, inplace=True)


        # Step3: SNV sum
        df1_out = self._sum_SNV(df1_filtered)
        df2_out = self._sum_SNV(df2_filtered)

        return df1_out, df2_out
    

    def _sum_SNV(self, df:pd.DataFrame) -> pd.DataFrame:

        df = df[['SNV_var', 'AA_var', 'control', 'test']].copy()

        final_list_aavar = list()
        final_dict_ctrl  = dict()
        final_dict_test  = dict()

        for i in range(len(df)):
            SNV_var = df.iloc[i]['SNV_var']
            AA_var  = df.iloc[i]['AA_var']
            ctrl = df.iloc[i]['control']
            test = df.iloc[i]['test']

            if SNV_var in final_dict_ctrl:
                final_dict_ctrl[SNV_var] = final_dict_ctrl[SNV_var] + ctrl
                final_dict_test[SNV_var] = final_dict_test[SNV_var] + test
            else:
                final_dict_ctrl[SNV_var] = ctrl
                final_dict_test[SNV_var] = test
                final_list_aavar.append(AA_var)
        
        df_normalized = pd.DataFrame({
            'SNV_var':final_dict_ctrl.keys(), 
            'AA_var' : final_list_aavar,
            'control': final_dict_ctrl.values(), 
            'test'   : final_dict_test.values(),
            })
        
        return df_normalized


class Normalizer:
    def __init__(self, ):

        pass


    def _make_variants_info(self, df:pd.DataFrame, control:str='control', test:str='test') -> pd.DataFrame:

        df_out = df.copy()

        #1 : Get raw log2-fold change (raw_LFC).
        df_out['raw_LFC'] = np.log2(((df_out[test] + 1) / (df_out[control] + 1)))
        
        #2 : Retrieve position information.
        df_out['var_pos'] = [id.split('pos')[-1][:-3] for id in df_out['SNV_var']]
        
        #3 : Add SNV mutation class label (Nonsense/missense/Synonymous). Requires reference info.
        list_mut_type = []
        for aa_var in df_out['AA_var']:
            if   aa_var.endswith('Stop'): list_mut_type.append('Nonsense')
            elif aa_var[0] == aa_var[-1]: list_mut_type.append('Synonymous')
            else: list_mut_type.append('Missense')
                
        df_out['mut_type'] = list_mut_type

        return df_out


    def lowess(self, data:pd.DataFrame, frac:float=0.15, control:str='control', test:str='test') -> pd.DataFrame:
        """The result of performing LOWESS normalization on the SNV sum count file.

        Args:
            data (pd.DataFrame): _description_
            exclude (_type_, optional): _description_. Defaults to None.
            frac (float, optional): _description_. Defaults to 0.15.

        Returns:
            pd.DataFrame: _description_
        """

        df_snv_sum = data.copy()
        df_snv_sum = self._make_variants_info(df_snv_sum, control, test)
        
        # step2 : Lowess regression
        df_syn = df_snv_sum[df_snv_sum['mut_type']=='Synonymous'].copy().reset_index(drop=True)

        x = np.array(df_syn['var_pos'], dtype=np.int64)
        y = np.array(df_syn['raw_LFC'], dtype=np.float64)

        syn_std = np.std(df_syn['raw_LFC'])

        lowess_model = lowess.Lowess()
        lowess_model.fit(x, y, frac=frac)

        x_pred = np.array([pos for pos in df_snv_sum['var_pos']], dtype=np.int64)
        y_pred = lowess_model.predict(x_pred)


        df_nor = df_snv_sum.copy()
        df_nor[f'lws_reg'] = y_pred
        df_nor[f'normalized_LFC'] = (df_nor['raw_LFC'] - df_nor[f'lws_reg']) / syn_std

        return df_nor
    
    
    def zscore(self, df:pd.DataFrame, control:str='control', test:str='test') -> pd.DataFrame:

        df_nor = df.copy()
        df_nor = self._make_variants_info(df_nor)

        std = np.std(df_nor['raw_LFC'])
        m   = np.mean(df_nor['raw_LFC'])

        df_nor['normalized_LFC'] = [(df_nor.iloc[i]['raw_LFC']-m)/std for i in range(len(df_nor))]

        return df_nor


def combine_data(files:list) -> pd.DataFrame:
    list_df = [pd.read_csv(file) for file in files]
    return pd.concat(list_df, axis = 0).reset_index(drop=True)


class VariantScore:
    def __init__(self):


        pass

    def calculate(self, replicate_1:str, replicate_2:str, var_type:str='SNV', sensitive_cutoff:int=0.95, resistant_cutoff:int=0.997) -> pd.DataFrame:
        """A method for calculating Adjusted LFC or Resistance score. It requires paths to two replicate files as input.
        You can choose var_type as SNV or AA to decide whether to calculate Adjusted LFC or resistance score.
        It automatically performs drug response classification based on the calculated score.

        Args:
            replicate_1 (str): _description_
            replicate_2 (str): _description_
            var_type (str, optional): SNV 또는 AA 중에 선택할 수 있다. Defaults to 'SNV'.
            sensitive_cutoff (int, optional): Sensitive-Intermediate 구분에 사용되는 synonymous score cut-off. Defaults to 0.95.
            resistant_cutoff (int, optional): Resistant-Intermediate 구분에 사용되는 synonymous score cut-off. Defaults to 0.997.

        Raises:
            ValueError: Occurs when an invalid value is entered for `var_type`.

        Returns:
            pd.DataFrame: _description_
        """        

        self.sens_cutoff = sensitive_cutoff
        self.res_cutoff  = resistant_cutoff

        if var_type == 'SNV':
            df_out  = self._get_adjusted_lfc(replicate_1, replicate_2)
        elif var_type == 'AA':
            df_out  = self._get_adjusted_lfc(replicate_1, replicate_2)
            df_out = self._get_resistance_score(df_out)
        else:
            raise ValueError('Not available variants type. Please select "SNV" or "AA".')
        
        return df_out


    def _get_adjusted_lfc(self, replicate_1:str, replicate_2:str):

        df1  = pd.read_csv(replicate_1).set_index('SNV_var').rename(
            columns={'raw_LFC': 'raw_LFC_1', 'normalized_LFC': 'nLFC_1'})
        
        df2  = pd.read_csv(replicate_2).set_index('SNV_var').rename(
            columns={'raw_LFC': 'raw_LFC_2', 'normalized_LFC': 'nLFC_2'})[['raw_LFC_2', 'nLFC_2']]

        df_merge = pd.concat([df1, df2], axis=1)
        df_merge['Adjusted_LFC'] = df_merge[['nLFC_1', 'nLFC_2']].mean(axis=1)

        df_merge = self._classification(df_merge)

        return df_merge
    
    def _get_resistance_score(self, df_adjLFC:pd.DataFrame):

        df = df_adjLFC.copy()

        dict_mut_type = {}
        for idx in df.index:
            data = df.loc[idx]
            dict_mut_type[data['AA_var']] = data['mut_type']

        aa_df = df.groupby('AA_var').mean(numeric_only=True)[['raw_LFC_1', 'raw_LFC_2', 'nLFC_1', 'nLFC_2']]
        
        aa_df['Resistance_score'] = aa_df[['nLFC_1', 'nLFC_2']].mean(axis = 1)
        aa_df['mut_type'] = [dict_mut_type[AA_var] for AA_var in aa_df.index]

        aa_df = self._classification(aa_df)

        return aa_df
    

    def _classification(self, df:pd.DataFrame) -> pd.DataFrame:
        # Integrate classification directly into the pipeline for calculating LFC.
        synonymous_df = df[df['mut_type']=='Synonymous'].copy()
        resistant_1  = synonymous_df['nLFC_1'].quantile(self.res_cutoff)
        resistant_2  = synonymous_df['nLFC_2'].quantile(self.res_cutoff)
        senseitive_1 = synonymous_df['nLFC_1'].quantile(self.sens_cutoff)
        senseitive_2 = synonymous_df['nLFC_2'].quantile(self.sens_cutoff)

        def label(row):
            if row['nLFC_1'] > resistant_1 and row['nLFC_2'] > resistant_2:
                return 'Resistant'
            elif row['nLFC_1'] < senseitive_1 and row['nLFC_2'] < senseitive_2 :
                return 'Sensitive'
            else:
                return 'Intermediate'
            
        df['Classification'] = df.apply(label, axis = 1)

        return df


class ReadPatternAnalyzer:
    def __init__(self,):
        """Classification based on mutation type:
        Divide and save files according to substitution/insertion/deletion/complex.

        Case1: If only '-' is present in Aligned_Sequence > Deletion
        Case2: If only '-' is present in Reference_Sequence > Insertion
        Case3: If neither has '-' > Substitution (or WT)
        Case4: Otherwise, multiple mutations present together > Complex
        """        

        pass

    
    def run(self, freq_table:str, ref_info:str) -> pd.DataFrame:
        """Executing a pipeline to analyze read patterns based on completed CRISPResso2 analysis files.

        Args:
            freq_table (str): Frequency table generated from CRISPResso2 results.
            ref_info (str): White list prepared for expected reads.

        Returns:
            pd.DataFrame: Read pattern 정보가 추가된 DataFrame
        """        

        df_freq = pd.read_csv(freq_table, sep='\t')
        df_ref = pd.read_csv(ref_info)

        is_ins = df_freq['Reference_Sequence'].str.contains('-')
        is_del = df_freq['Aligned_Sequence'].str.contains('-')

        df_ins = df_freq[is_ins & ~is_del]
        df_del = df_freq[~is_ins & is_del]
        df_sub = df_freq[~is_ins & ~is_del]

        list_idx  = list(df_sub.index) + list(df_ins.index) + list(df_del.index)
        df_complx = df_freq.drop(list_idx)
        df_complx['mut_type']  = 'Complex'
        df_complx['mut_class'] = 'Complex'

        # classification
        df_sub_type = self._classify_substitutions(df_sub, df_ref)
        df_ins_type = self._classify_indel(df_ins, 'insertion')
        df_del_type = self._classify_indel(df_del, 'deletion')

        df_merge = pd.concat([df_sub_type, df_ins_type, df_del_type, df_complx])
        df_merge = df_merge.sort_index()

        df_merge = df_merge[['#Reads', '%Reads', 'mut_type', 'mut_class']]
        
        return df_merge


    def _count_mismatches(self, seq1:str, seq2:str) -> int:
        """Compare 2 sequences and return the number of mismatches"""

        cnt = 0

        for a, b in zip(list(seq1), list(seq2)):
            if a != b: cnt += 1
        
        return cnt


    def _classify_substitutions(self, df_reads:pd.DataFrame, df_ref:pd.DataFrame):
        """Separating instances where only substitutions occurred into those where WT and SynPrime accurately occurred, 
        and classifying other instances containing unintended edits into a separate DataFrame.

        Args:
            df_reads (pd.DataFrame): _description_
            df_ref (pd.DataFrame): _description_

        Returns:
            _type_: _description_
        """        

        # Step1: make dictionary for read count
        dict_ref = {}

        for idx in df_ref.index:
            dict_ref[df_ref.iloc[idx].RefSeq] = df_ref.iloc[idx].Label

        # Step2: classify substitution types
        list_sub_type = []
        list_sub_class= []

        for idx in tqdm(df_reads.index, total=len(df_reads.index), 
                        desc='Classify pattern', ncols=100, leave=False):

            data = df_reads.loc[idx]

            try: 
                mut_type = dict_ref[data.Aligned_Sequence]
                list_sub_type.append(mut_type)

                if mut_type in ['Intended_only', 'Synony_only']: 
                    list_sub_class.append('Single_edit')
                else:
                    list_sub_class.append(dict_ref[data.Aligned_Sequence])
            
            # SynPrime으로 생길 수 없는 product에 대해서 분류
            except:
                cnt = self._count_mismatches(data.Aligned_Sequence, data.Reference_Sequence)
                list_sub_type.append(f'sub{cnt}')
                
                if cnt > 4: list_sub_class.append(f'sub5more')
                else      : list_sub_class.append(f'sub{cnt}')


        df_reads_type = df_reads.copy()
        df_reads_type['mut_type'] = list_sub_type
        df_reads_type['mut_class'] = list_sub_class

        return df_reads_type
        
    def _find_indel_sequence(self, aligned_seq:str, ref_seq:str, type:str='insertion') -> str:
        """Handler for analyzing insertion/deletion patterns in aligned reads.

        Args:
            aligned_seq (str): _description_
            ref_seq (str): _description_
            type (str): Select'insertion' or 'deletion'

        Returns:
            str: Sequence of insertion or deletion.
        """

        if   type == 'insertion': pass
        elif type == 'deletion' :
            aligned_seq, ref_seq = ref_seq, aligned_seq
        else: 
            raise ValueError('Not available type. Select "insertion" or "deletion"')

        list_ins = []
        isInsert = False

        for a, b in zip(list(aligned_seq), list(ref_seq)):

            if b == '-':
                isInsert = True
                list_ins.append(a)

            else:
                if isInsert == False: pass
                else: break

        return ''.join(list_ins)

    def _classify_indel(self, df_reads:pd.DataFrame, type:str):
        """Code for analyzing insertion/deletion patterns in the frequency table read.

        Args:
            df_reads (pd.DataFrame): _description_
            type (str): Select'insertion' or 'deletion'

        Returns:
            _type_: _description_
        """        

        # make list for inserted seq
        list_indel_seq = []

        for idx in df_reads.index:

            data = df_reads.loc[idx]
            aligned_seq = data.Aligned_Sequence
            ref_seq     = data.Reference_Sequence

            # Find inserted sequence
            indel_seq = self._find_indel_sequence(aligned_seq, ref_seq, type)
            
            list_indel_seq.append(f'{type[:3]}{len(indel_seq)}:'+indel_seq)

        df_out = df_reads.copy()
        df_out['mut_type'] = list_indel_seq
        df_out['mut_class'] = type[:3]

        return df_out


def single_clone_var_freq(sample_id:str, freq_table:str, wt_seq:str, edit_seq:str, intended_only:str) -> pd.DataFrame:
    
    wt_seq   = wt_seq.upper()
    edit_seq = edit_seq.upper()
    
    # Step1: read CRISPResso aligned & reference file
    df = pd.read_csv(freq_table, sep = '\t')
    
    sample_name = os.path.basename(freq_table).replace('.txt', '')
    dict_out = {wt_seq: 0, edit_seq: 0, intended_only:0, 'Others': 0}

    print(f'[Info] Start - {sample_name}')

    # Step2: read count
    for i in df.index:
        data = df.iloc[i]
        seq  = data['Aligned_Sequence']
        cnt  = int(data['#Reads'])
        
        try   : dict_out[seq] += cnt
        except: dict_out['Others'] += cnt

    dict_out['WT'] = dict_out.pop(wt_seq)
    dict_out['Variant'] = dict_out.pop(edit_seq)
    
    try   : dict_out['Intended_only'] = dict_out.pop(intended_only)
    except: dict_out['Intended_only'] = 0

    df_out = pd.DataFrame.from_dict(dict_out, orient='index').T
    df_out = df_out.rename(index={0: sample_id})

    return df_out