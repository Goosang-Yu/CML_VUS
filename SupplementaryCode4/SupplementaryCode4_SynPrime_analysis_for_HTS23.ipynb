{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Supplementary Code 4**\n",
    "This notebook was used for analysis of NGS reads containing intended prime-editing and synonymous mutation marker. For more detail, please read Methods and Supplementary Information. \n",
    "\n",
    "Lead contact: Hyoungbum Henry Kim (hkim1@gmail.com)\n",
    "\n",
    "Technical contact: Goosang Yu (gsyu93@gmail.com), Yusang Jung (ys.jung@yuhs.ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory tree\n",
    "\n",
    "ðŸ“¦Working directory  \n",
    " â”£ ðŸ“‚data  \n",
    " â”ƒ â”£ ðŸ“‚NGS_FASTQ_files  \n",
    " â”ƒ â”£ ðŸ“‚NGS_frequency_table  \n",
    " â”ƒ â”ƒ â”£ ðŸ“œC4Bosutinib791.txt  \n",
    " â”ƒ â”ƒ â”£ ðŸ“œC4Control797.txt  \n",
    " â”ƒ â”ƒ â”— ðŸ“œ...  \n",
    " â”ƒ â”£ ðŸ“‚read_counts  \n",
    " â”ƒ â”£ ðŸ“‚statistics  \n",
    " â”ƒ  \n",
    " â”£ ðŸ“‚src  \n",
    " â”ƒ â”£ ðŸ“œAlignment.py  \n",
    " â”ƒ â”£ ðŸ“œVarCalling.py  \n",
    " â”ƒ  \n",
    " â”£ ðŸ“‚variants_info  \n",
    " â”ƒ â”£ ðŸ“œex4_info.csv  \n",
    " â”ƒ â”£ ðŸ“œex5_info.csv  \n",
    " â”ƒ â”£ ðŸ“œex6_info.csv  \n",
    " â”ƒ â”£ ðŸ“œex7_info.csv  \n",
    " â”ƒ â”£ ðŸ“œex8_info.csv  \n",
    " â”ƒ â”£ ðŸ“œex9_info.csv  \n",
    " â”ƒ â”£ ðŸ“œinvivo_ex4_info.csv  \n",
    " â”ƒ â”— ðŸ“œinvivo_ex9_info.csv  \n",
    " â”ƒ  \n",
    " â”— ðŸ“œSuppleCode4.ipynb (this file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- CRISPResso2 (>= 2.x.x)\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants calling and make read count file\n",
    "After running CRISPResso, generate the read count file. This is the process of creating the foundational file for all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from src.Alignment import ABL1VUS\n",
    "from src.VarCalling import make_count_file, read_statistics, combine_data, VariantFilter, VariantScore, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Info] Read counting: K562PE4K_HTS2Dose0.5xDay10_Exon5_Rep1_Asciminib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139161/139161 [00:13<00:00, 10209.21it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose0.5xDay10_Exon5_Rep1_Imatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143326/143326 [00:14<00:00, 10022.73it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose0.5xDay10_Exon5_Rep2_Asciminib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210654/210654 [00:20<00:00, 10350.57it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose0.5xDay10_Exon5_Rep2_Imatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174170/174170 [00:16<00:00, 10468.49it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep1_Asciminib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 130389/130389 [00:13<00:00, 10009.01it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep1_Bosutinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145079/145079 [00:14<00:00, 10106.44it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep1_Dasatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146242/146242 [00:14<00:00, 10209.32it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep1_Imatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153268/153268 [00:14<00:00, 10394.07it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep1_Nilotinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147738/147738 [00:14<00:00, 10010.73it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep1_Ponatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147519/147519 [00:14<00:00, 10257.34it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep2_Asciminib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179134/179134 [00:17<00:00, 10371.98it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep2_Bosutinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150729/150729 [00:14<00:00, 10433.21it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep2_Dasatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162760/162760 [00:16<00:00, 9937.91it/s] \n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep2_Imatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158730/158730 [00:15<00:00, 10047.92it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep2_Nilotinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156467/156467 [00:15<00:00, 10254.07it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose1xDay10_Exon5_Rep2_Ponatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154243/154243 [00:14<00:00, 10355.28it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose2xDay10_Exon5_Rep1_Asciminib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131581/131581 [00:12<00:00, 10348.62it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose2xDay10_Exon5_Rep1_Imatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94685/94685 [00:09<00:00, 10520.52it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose2xDay10_Exon5_Rep2_Asciminib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 178982/178982 [00:17<00:00, 10403.37it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2Dose2xDay10_Exon5_Rep2_Imatinib: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156344/156344 [00:15<00:00, 10200.53it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2DoseControlDay10_Exon5_Rep1_DMSO: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142218/142218 [00:13<00:00, 10232.31it/s]\n",
      "[Info] Read counting: K562PE4K_HTS2DoseControlDay10_Exon5_Rep2_DMSO: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158171/158171 [00:15<00:00, 10366.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make count files from frequency table\n",
    "\n",
    "freq_tables = glob('data/frequency_table/*HTS2*.txt')\n",
    "\n",
    "for f in freq_tables:\n",
    "\n",
    "    n_sample = os.path.basename(f).replace('.txt', '')\n",
    "    exon_num = n_sample.split('Exon')[1][0]\n",
    "    ref_info = f'variants_info/ex{exon_num}_info.csv'\n",
    "\n",
    "    \n",
    "    df_cnt = make_count_file(f, ref_info)\n",
    "    df_cnt.to_csv(f'data/read_counts/Count_{n_sample}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance analysis\n",
    "Compare the prime-edited sample with the unedited sample to calculate the odds ratio and Fisher's exact test p-value for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis: Count_K562PE4K_HTS2DoseControlDay10_Exon5_Rep1_DMSO\n",
      "Analysis: Count_K562PE4K_HTS2DoseControlDay10_Exon5_Rep2_DMSO\n",
      "Analysis: Count_K562PE4K_HTS3DoseControlDay4_Exon5_Rep1_DMSO\n",
      "Analysis: Count_K562PE4K_HTS3DoseControlDay4_Exon5_Rep2_DMSO\n",
      "Analysis: Count_K562PE4K_HTS3DoseControlDay6_Exon5_Rep1_DMSO\n",
      "Analysis: Count_K562PE4K_HTS3DoseControlDay6_Exon5_Rep2_DMSO\n",
      "Analysis: Count_K562PE4K_HTS3DoseControlDay10_Exon5_Rep1_DMSO\n",
      "Analysis: Count_K562PE4K_HTS3DoseControlDay10_Exon5_Rep2_DMSO\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics with read count for each variants\n",
    "\n",
    "list_HTS23 = [\n",
    "    # HTS2\n",
    "    'K562PE4K_HTS2DoseControlDay10_Exon5_Rep1_DMSO',\n",
    "    'K562PE4K_HTS2DoseControlDay10_Exon5_Rep2_DMSO',\n",
    "\n",
    "    # HTS3\n",
    "    'K562PE4K_HTS3DoseControlDay4_Exon5_Rep1_DMSO',\n",
    "    'K562PE4K_HTS3DoseControlDay4_Exon5_Rep2_DMSO',\n",
    "    'K562PE4K_HTS3DoseControlDay6_Exon5_Rep1_DMSO',\n",
    "    'K562PE4K_HTS3DoseControlDay6_Exon5_Rep2_DMSO',\n",
    "    'K562PE4K_HTS3DoseControlDay10_Exon5_Rep1_DMSO',\n",
    "    'K562PE4K_HTS3DoseControlDay10_Exon5_Rep2_DMSO',\n",
    "]\n",
    "\n",
    "for sample in list_HTS23:\n",
    "    \n",
    "    test_file       = f'data/read_counts/Count_{sample}.csv'\n",
    "    background_file = f'data/read_counts/Count_K562PE4K_unedit_Exon5.csv'\n",
    "\n",
    "    df_stats = read_statistics(test_file, background_file)\n",
    "\n",
    "    df_stats.to_csv(f'data/statistics/Stat_{sample}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMSO vs TKI response analysis\n",
    "Analysis for resistance to drugs\n",
    "\n",
    "- Test: Making variants using Prime editing for 20 days, followed by 10 days of TKI treatment.\n",
    "- Control: Making variants using Prime editing for 20 days, followed by 10 days of DMSO treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sample_pair import dict_HTS2, dict_HTS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HTS2 sample\n",
    "\n",
    "for sample, data in dict_HTS2.items():\n",
    "    \n",
    "    test_1, test_2 = data['test']\n",
    "    cont_1, cont_2 = data['cont']\n",
    "    \n",
    "    vf = VariantFilter(\n",
    "        test_r1    = f'data/read_counts/Count_{test_1}.csv',\n",
    "        test_r2    = f'data/read_counts/Count_{test_2}.csv',\n",
    "        control_r1 = f'data/statistics/Stat_{cont_1}.csv',\n",
    "        control_r2 = f'data/statistics/Stat_{cont_2}.csv',\n",
    "    )\n",
    "    \n",
    "    df_rep1, df_rep2 = vf.filter(OR_cutoff=2, p_cutoff=0.05, rpm_cutoff=10)\n",
    "\n",
    "    normal = Normalizer()\n",
    "\n",
    "    # LOWESS regression normalization\n",
    "    lws_frac = 0.15\n",
    "    \n",
    "    df_nor1 = normal.lowess(df_rep1, frac=lws_frac)\n",
    "    df_nor2 = normal.lowess(df_rep2, frac=lws_frac)\n",
    "\n",
    "    df_nor1.to_csv(f'data/statistics/Filtered_{test_1}.csv', index=False)\n",
    "    df_nor2.to_csv(f'data/statistics/Filtered_{test_2}.csv', index=False)\n",
    "\n",
    "    normalized_rep_1 = f'data/statistics/Filtered_{test_1}.csv'\n",
    "    normalized_rep_2 = f'data/statistics/Filtered_{test_2}.csv'\n",
    "\n",
    "    # Score calculation í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    score = VariantScore()\n",
    "\n",
    "    adjus_LFC = score.calculate(normalized_rep_1, normalized_rep_2, var_type='SNV')\n",
    "    res_score = score.calculate(normalized_rep_1, normalized_rep_2, var_type='AA')\n",
    "\n",
    "    n_sample = test_1.replace('Rep1_', '')\n",
    "\n",
    "    adjus_LFC.to_csv(f'data/adjusted_LFC/AdjustedLFC_{n_sample}.csv')\n",
    "    res_score.to_csv(f'data/resistance_score/ResistanceScore_{n_sample}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HTS3 sample\n",
    "\n",
    "for sample, data in dict_HTS3.items():\n",
    "    \n",
    "    test_1, test_2 = data['test']\n",
    "    cont_1, cont_2 = data['cont']\n",
    "    \n",
    "    vf = VariantFilter(\n",
    "        test_r1    = f'data/read_counts/Count_{test_1}.csv',\n",
    "        test_r2    = f'data/read_counts/Count_{test_2}.csv',\n",
    "        control_r1 = f'data/statistics/Stat_{cont_1}.csv',\n",
    "        control_r2 = f'data/statistics/Stat_{cont_2}.csv',\n",
    "    )\n",
    "    \n",
    "    df_rep1, df_rep2 = vf.filter(OR_cutoff=2, p_cutoff=0.05, rpm_cutoff=10)\n",
    "\n",
    "    normal = Normalizer()\n",
    "\n",
    "    # LOWESS regression normalization\n",
    "    lws_frac = 0.15\n",
    "    \n",
    "    df_nor1 = normal.lowess(df_rep1, frac=lws_frac)\n",
    "    df_nor2 = normal.lowess(df_rep2, frac=lws_frac)\n",
    "\n",
    "    df_nor1.to_csv(f'data/statistics/Filtered_{test_1}.csv', index=False)\n",
    "    df_nor2.to_csv(f'data/statistics/Filtered_{test_2}.csv', index=False)\n",
    "\n",
    "    normalized_rep_1 = f'data/statistics/Filtered_{test_1}.csv'\n",
    "    normalized_rep_2 = f'data/statistics/Filtered_{test_2}.csv'\n",
    "\n",
    "    # Score calculation í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    score = VariantScore()\n",
    "\n",
    "    adjus_LFC = score.calculate(normalized_rep_1, normalized_rep_2, var_type='SNV')\n",
    "    res_score = score.calculate(normalized_rep_1, normalized_rep_2, var_type='AA')\n",
    "\n",
    "    n_sample = test_1.replace('Rep1_', '')\n",
    "\n",
    "    adjus_LFC.to_csv(f'data/adjusted_LFC/AdjustedLFC_{n_sample}.csv')\n",
    "    res_score.to_csv(f'data/resistance_score/ResistanceScore_{n_sample}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
